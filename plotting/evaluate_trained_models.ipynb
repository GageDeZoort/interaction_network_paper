{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "sys.path.append('../')\n",
    "\n",
    "import pickle\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from models.dataset import Dataset\n",
    "from models.interaction_network import InteractionNetwork\n",
    "from models.graph import Graph, save_graphs, load_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font',**{'family':'sans-serif','sans-serif':['DejaVu Sans'],'size':10})\n",
    "\n",
    "# Set the font used for MathJax - more on this later\n",
    "plt.rc('mathtext',**{'default':'regular'})\n",
    "colors = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "          '#f781bf', '#a65628', '#984ea3',\n",
    "          '#999999', '#e41a1c', '#dede00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained models for the paper \n",
    "model_dir = '/tigress/jdezoort/IN_paper_models/'\n",
    "models = os.listdir('/tigress/jdezoort/IN_paper_models/')\n",
    "model_paths = [model_dir+model for model in models]\n",
    "\n",
    "# initial discriminants (won't matter in the end)\n",
    "discs = {'2': 0.346, '1p5': 0.38, '1': 0.225, '0p75': 0.203, '0p6': 0.2}\n",
    "device = \"cpu\"\n",
    "\n",
    "# track the losses and accuracies per model per training set\n",
    "# 5 trained models for 2, 1.5, 1, 0.75, and 0.6 GeV graphs \n",
    "# 6 train sets for 2, 1.5, 1, 0.75, and 0.6 GeV graphs \n",
    "overall_losses = np.zeros((5,6))\n",
    "overall_losses_std = np.zeros((5,6))\n",
    "overall_accs = np.zeros((5,6))\n",
    "overall_accs_std = np.zeros((5,6))\n",
    "overall_tpr = np.zeros((5,6))\n",
    "overall_tpr_std = np.zeros((5,6))\n",
    "overall_tnr = np.zeros((5,6))\n",
    "overall_tnr_std = np.zeros((5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: /tigress/jdezoort/IN_paper_models/train1_40hu_heptrkx_plus_epoch30_0p75GeV.pt\n",
      "Testing pt=0p6 GeV\n",
      "Model: /tigress/jdezoort/IN_paper_models/train1_40hu_heptrkx_plus_epoch60_2GeV.pt\n",
      "Testing pt=0p6 GeV\n",
      "Model: /tigress/jdezoort/IN_paper_models/train1_40hu_heptrkx_plus_epoch48_1GeV.pt\n",
      "Testing pt=0p6 GeV\n",
      "Model: /tigress/jdezoort/IN_paper_models/train1_40hu_heptrkx_plus_epoch9_0p6GeV.pt\n",
      "Testing pt=0p6 GeV\n",
      "Model: /tigress/jdezoort/IN_paper_models/train1_40hu_heptrkx_plus_epoch60_1p5GeV.pt\n",
      "Testing pt=0p6 GeV\n",
      "Sampling graphs from: ../../hitgraphs_2/heptrkx_plus_0p6/\n",
      "...running validation\n",
      "...validation produces best disc 0.008+-0.004\n",
      "...testing performance\n",
      "\n",
      " --> Results:\n",
      " --> overall loss 0.0360+-0.001792, overall acc 0.9799+-0.0018\n",
      " --> overall tpr 0.9792+-0.005641, overall tnr 0.9799+-0.0021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(model_paths): # loop over each of the 5 trained models \n",
    "    print(\"Model:\", model)\n",
    "    \n",
    "    # for each model, loop over graphs at each pt_min\n",
    "    for j, test_pt in enumerate(['0p6']):\n",
    "        print(\"Testing pt={} GeV\".format(test_pt))\n",
    "        \n",
    "        # load up the correct model \n",
    "        model_pt = model.split('.')[0].split('_')[-1].strip('GeV')\n",
    "        if model_pt != '1p5': continue\n",
    "        disc = discs[model_pt]\n",
    "        interaction_network = InteractionNetwork(3, 4, 4)\n",
    "        interaction_network.load_state_dict(torch.load(model, map_location=torch.device('cpu')))\n",
    "        interaction_network.eval()\n",
    "        \n",
    "        # load the graphs, making sure they belong to train_2 \n",
    "        construction = 'heptrkx_plus'\n",
    "        graph_indir = \"../../hitgraphs_2/{}_{}/\".format(construction, test_pt)\n",
    "        print('Sampling graphs from:', graph_indir)\n",
    "        graph_files = np.array(os.listdir(graph_indir))\n",
    "        train_2_mask = [(int(graph_file.split(\"00000\")[1].split(\"_\")[0]) > 2820)\n",
    "                        for graph_file in graph_files]\n",
    "        graph_files = graph_files[train_2_mask]\n",
    "        n_graphs = len(graph_files)\n",
    "    \n",
    "        # randomly partition the graphs into 100 validation and 600 testing graphs \n",
    "        IDs = np.arange(n_graphs)\n",
    "        np.random.shuffle(IDs)\n",
    "        \n",
    "        \n",
    "        partition = {'val': graph_files[IDs[500:600]],\n",
    "                     'test':  graph_files[IDs[1000:1500]]}\n",
    "        \n",
    "        # create a validation dataloader \n",
    "        params = {'batch_size': 1, 'shuffle': True, 'num_workers': 6}\n",
    "        if (test_pt=='0p5'): params['num_workers'] = 1\n",
    "        val_set = Dataset(graph_indir, partition['val']) \n",
    "        val_loader = torch.utils.data.DataLoader(val_set, **params)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            print(\"...running validation\")\n",
    "           \n",
    "            # run validation procedure to determine the best discriminant to use\n",
    "            best_discs=[]\n",
    "            for data, target in val_loader:\n",
    "                # grab data and targets \n",
    "                X, Ra = data['X'].float().to(device), data['Ra'].float().to(device)\n",
    "                Ri, Ro = data['Ri'].float().to(device), data['Ro'].float().to(device)\n",
    "                #pids = target['pid'][0].to(device)\n",
    "                target = target['y'].to(device)\n",
    "        \n",
    "                # inference, loss calculation\n",
    "                output = interaction_network(X, Ra.float(), Ri.float(), Ro.float())\n",
    "                test_loss = F.binary_cross_entropy(output.squeeze(2), target,\n",
    "                                           reduction='mean').item()\n",
    "                \n",
    "                # the best discriminant balances true positives and true negatives\n",
    "                diff, best_disc = 100, 0\n",
    "                best_tpr, best_tnr = 0, 0\n",
    "                for disc in np.arange(0, 0.6, 0.001):\n",
    "                    true_pos = ((target==1).squeeze() & (output>disc).squeeze())\n",
    "                    true_neg = ((target==0).squeeze() & (output<disc).squeeze())\n",
    "                    false_pos = ((target==0).squeeze() & (output>disc).squeeze())\n",
    "                    false_neg = ((target==1).squeeze() & (output<disc).squeeze())\n",
    "                    N_tp, N_tn = torch.sum(true_pos).item(), torch.sum(true_neg).item()\n",
    "                    N_fp, N_fn = torch.sum(false_pos).item(), torch.sum(false_neg).item()\n",
    "                    true_pos_rate = N_tp/(N_tp + N_fn)\n",
    "                    true_neg_rate = N_tn/(N_tn + N_fp)\n",
    "                    delta = abs(true_pos_rate - true_neg_rate)\n",
    "                    if (delta < diff):\n",
    "                        diff, best_disc = delta, disc\n",
    "                        \n",
    "                best_discs.append(best_disc)\n",
    "                del X\n",
    "                del Ra\n",
    "                del Ri\n",
    "                del Ro\n",
    "                del target\n",
    "            disc = np.mean(best_discs)\n",
    "            print(\"...validation produces best disc {:.3f}+-{:.3f}\".format(disc, np.std(best_discs)))\n",
    "\n",
    "        # create a test dataloader \n",
    "        params = {'batch_size': 1, 'shuffle': True, 'num_workers': 6}\n",
    "        if (test_pt=='0p5'): params['num_workers'] = 1\n",
    "        test_set = Dataset(graph_indir, partition['test']) \n",
    "        test_loader = torch.utils.data.DataLoader(test_set, **params)\n",
    "    \n",
    "        # for each training graph, track loss, accuracy, true positive rate, and true negative rate\n",
    "        losses, accs = [], []\n",
    "        true_pos_rates, true_neg_rates = [], []\n",
    "        with torch.no_grad():\n",
    "            print(\"...testing performance\")\n",
    "            \n",
    "            for data, target in test_loader:\n",
    "        \n",
    "                # grab data and targets \n",
    "                X, Ra = data['X'].float().to(device), data['Ra'].float().to(device)\n",
    "                Ri, Ro = data['Ri'].float().to(device), data['Ro'].float().to(device)\n",
    "                #pids = target['pid'][0].to(device)\n",
    "                target = target['y'].to(device)\n",
    "        \n",
    "                # inference, loss and acc calculations\n",
    "                output = interaction_network(X, Ra.float(), Ri.float(), Ro.float())\n",
    "                test_loss = F.binary_cross_entropy(output.squeeze(2), target,\n",
    "                                           reduction='mean').item()\n",
    "                accuracy = torch.sum(((target==1).squeeze() &\n",
    "                                      (output>disc).squeeze()) |\n",
    "                                     ((target==0).squeeze() &\n",
    "                                      (output<disc).squeeze())).float()/target.shape[1]\n",
    "                losses.append(test_loss)\n",
    "                accs.append(accuracy)\n",
    "                \n",
    "                # true positive rate, true negative rate calculations \n",
    "                true_pos = ((target==1).squeeze() & (output>disc).squeeze())\n",
    "                true_neg = ((target==0).squeeze() & (output<disc).squeeze())\n",
    "                false_pos = ((target==0).squeeze() & (output>disc).squeeze())\n",
    "                false_neg = ((target==1).squeeze() & (output<disc).squeeze())\n",
    "                N_tp, N_tn = torch.sum(true_pos).item(), torch.sum(true_neg).item()\n",
    "                N_fp, N_fn = torch.sum(false_pos).item(), torch.sum(false_neg).item()\n",
    "                true_pos_rate = N_tp/(N_tp + N_fn)\n",
    "                true_neg_rate = N_tn/(N_tn + N_fp)\n",
    "                \n",
    "                true_pos_rates.append(true_pos_rate)\n",
    "                true_neg_rates.append(true_neg_rate)\n",
    "                \n",
    "                del X\n",
    "                del Ra\n",
    "                del Ri\n",
    "                del Ro\n",
    "                del target\n",
    "                \n",
    "        # fill the global tables \n",
    "        overall_losses[i][j] = np.mean(losses)\n",
    "        overall_losses_std[i][j] = np.std(losses)\n",
    "        overall_accs[i][j] = np.mean(accs)\n",
    "        overall_accs_std[i][j] = np.std(accs)\n",
    "        overall_tpr[i][j] = np.mean(true_pos_rates)\n",
    "        overall_tpr_std[i][j] = np.std(true_pos_rates)\n",
    "        overall_tnr[i][j] = np.mean(true_neg_rates)\n",
    "        overall_tnr_std[i][j] = np.std(true_neg_rates)\n",
    "        \n",
    "        print(\"\\n --> Results:\")\n",
    "        print(\" --> overall loss {:.4f}+-{:4f}, overall acc {:.4f}+-{:.4f}\"\n",
    "              .format(np.mean(losses), np.std(losses), np.mean(accs), np.std(accs)))\n",
    "        print(\" --> overall tpr {:.4f}+-{:4f}, overall tnr {:.4f}+-{:.4f}\\n\"\n",
    "              .format(np.mean(true_pos_rates), np.std(true_pos_rates), \n",
    "                      np.mean(true_neg_rates), np.std(true_neg_rates)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models in order 2, 0.75, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
